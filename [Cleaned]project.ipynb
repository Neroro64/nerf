{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L-g4gQ5ZSoY",
        "colab_type": "text"
      },
      "source": [
        "# DD2424 - Project\n",
        "This is an altered version of the method presented in NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\n",
        "\n",
        "[Project website](http://www.matthewtancik.com/nerf)\n",
        "\n",
        "Components not included in the notebook\n",
        "\n",
        "* 5D input including view directions\n",
        "* Hierarchical Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRXeWVzEZSoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP11u0XuZSol",
        "colab_type": "code",
        "outputId": "1b2ba2dd-8fb0-49cd-ebdd-b922b458af4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "if not os.path.exists('tiny_nerf_data.npz'):\n",
        "    !wget https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyvQrdR3c8BE",
        "colab_type": "code",
        "outputId": "81af5f76-acdd-4b56-c8eb-ab21d190a56f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\"\"\"\n",
        "Checks what gpu has been allocated\n",
        "\"\"\"\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMFNzBujZSot",
        "colab_type": "text"
      },
      "source": [
        "# Load input images and poses. \n",
        "The input images have size (100,100,3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxkvRBOxZSot",
        "colab_type": "code",
        "outputId": "4d7583e4-0e23-4331-83bd-6fa33d6b92c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "data = np.load('tiny_nerf_data.npz')\n",
        "images = data['images']\n",
        "poses = data['poses']\n",
        "focal = data['focal']\n",
        "H, W = images.shape[1:3]\n",
        "print(images.shape, poses.shape, focal)\n",
        "\n",
        "testimg, testpose = images[101], poses[101]\n",
        "images = images[:100,...,:3]\n",
        "poses = poses[:100]\n",
        "\n",
        "plt.imshow(testimg)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bZWjihrZSoz",
        "colab_type": "text"
      },
      "source": [
        "# NeRF network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I09jb6xXZSo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def posenc(x):\n",
        "  rets = [x]\n",
        "  for i in range(L_embed):\n",
        "    for fn in [tf.sin, tf.cos]:\n",
        "      rets.append(fn(2.**i * x))\n",
        "  return tf.concat(rets, -1)\n",
        "\n",
        "def get_rays(H, W, focal, c2w):\n",
        "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
        "    dirs = tf.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -tf.ones_like(i)], -1)\n",
        "    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
        "    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
        "    return rays_o, rays_d\n",
        "\n",
        "L_embed = 6\n",
        "embed_fn = posenc\n",
        "\n",
        "def init_model(D=8, W=256):\n",
        "    relu = tf.keras.layers.ReLU()    \n",
        "    dense = lambda W=W, act=relu : tf.keras.layers.Dense(W, activation=act)\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(3 + 3*2*L_embed)) \n",
        "    outputs = inputs\n",
        "    for i in range(D):\n",
        "        outputs = dense()(outputs)\n",
        "        if i%4==0 and i>0:\n",
        "            outputs = tf.concat([outputs, inputs], -1)\n",
        "    outputs = dense(4, act=None)(outputs)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom model and layer\n",
        "- 1 (Might work, but super slow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHoaAy2aVli-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MLP1(tf.keras.layers.Dense):\n",
        "    def __init__(self, W, activation, H=8, CR=False):\n",
        "      super(MLP, self).__init__(W, activation=activation)\n",
        "\n",
        "      self.cls_map = {}\n",
        "      self.H = H\n",
        "      self.CR = CR\n",
        "\n",
        "    def build(self, inputs):\n",
        "      super(MLP, self).build(inputs)\n",
        "      self.hash_mat = tf.random.uniform((inputs[1],self.H), minval=-1, maxval=1)\n",
        "    \n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs): \n",
        "      @tf.function\n",
        "      def func(vec):\n",
        "        vec = tf.reshape(vec, (1,-1))\n",
        "        digest = tf.matmul(vec, self.hash_mat)\n",
        "        digest = tf.cast(tf.greater(digest, 0), dtype=tf.int32)\n",
        "        key = tf.reduce_sum(digest * 2 ** tf.range(0, self.H)).ref()\n",
        "        cond = tf.cast(key in self.cls_map, tf.bool)\n",
        "        def in_table(): \n",
        "          self.cls_map.setdefault(key, vec)\n",
        "          return self.cls_map[key]\n",
        "        def not_in_table():\n",
        "          y = super(MLP, self).call(vec)\n",
        "          self.cls_map[key] = y\n",
        "          return y\n",
        "\n",
        "        row = tf.cond(cond, in_table, not_in_table)\n",
        "\n",
        "        return row\n",
        "      print(inputs.shape)\n",
        "      outputs = tf.map_fn(func, inputs, dtype=tf.float32)\n",
        "      return tf.reshape(outputs, (-1, inputs.shape[1]))\n",
        "  \n",
        "class neRF1(tf.keras.Model):\n",
        "  def init_model(self, D=8, W=256):\n",
        "    relu = tf.keras.layers.ReLU()    \n",
        "    dense = lambda W=W, act=relu : MLP(W, activation=act, H=self.H, CR=self.CR)\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(3 + 3*2*L_embed)) \n",
        "    outputs = inputs\n",
        "    for i in range(D):\n",
        "        outputs = dense()(outputs)\n",
        "        if i%4==0 and i>0:\n",
        "            outputs = tf.concat([outputs, inputs], -1)\n",
        "    outputs = dense(4, act=None)(outputs)\n",
        "    \n",
        "    return inputs, outputs\n",
        "  \n",
        "\n",
        "\n",
        "  def __init__(self, H=8, CR=False):\n",
        "    self.H = H\n",
        "    self.CR = CR\n",
        "    inputs, outputs = self.init_model()\n",
        "    super(neRF, self).__init__(inputs=inputs, outputs=outputs)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    return super(neRF, self).call(inputs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom model and layer\n",
        "- 2 Integrated the cls into the layers, not tested"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(tf.keras.layers.Dense):\n",
        "    def __init__(self, W, activation, H=10, CR=False):\n",
        "        super(MLP, self).__init__(W, activation=activation)\n",
        "        self.i = 2 ** tf.range(0, H)\n",
        "        self.H = H\n",
        "    def build(self, inputs):\n",
        "        super(MLP, self).build(inputs)\n",
        "        self.hash_mat = tf.keras.utils.normalize(tf.random.uniform((inputs[1],self.H), minval=-1, maxval=1), axis=0)\n",
        "        self.cluster_centers = tf.zeros((2**H, inputs[1]))\n",
        "\n",
        "    def call(self, inputs): \n",
        "        digest = tf.matmul(inputs, self.hash_mat)\n",
        "        digest = tf.cast(tf.greater(digest, 0), dtype=tf.int32)\n",
        "        keys = tf.reshape(tf.reduce_sum(digest * self.i, axis=1), (inputs.shape[0], 1))\n",
        "        keys = tf.concat([keys, tf.reshape(tf.range(0,inputs.shape[0]),(inputs.shape[0],1))], axis=1)\n",
        "        keys = tf.cast(keys, dtype=tf.int64)\n",
        "        st = tf.sparse.SparseTensor(keys, values=tf.ones(keys.shape[0]), dense_shape=(2**self.H, inputs.shape[0]))\n",
        "        centroids = tf.math.divide_no_nan(tf.sparse.sparse_dense_matmul(self.st, inputs), tf.reshape(tf.sparse.reduce_sum(self.st, axis=1), (self.cluster_centers.shape[0],1)))\n",
        "        self.cluster_centers = (self.cluster_centers + centroids) / 2\n",
        "        y = super(MLP, self).call(self.cluster_centers)\n",
        "        outputs = tf.sparse.sparse_dense_matmul(tf.sparse.transpose(st), inputs)\n",
        "        return outputs\n",
        "\n",
        "  \n",
        "class neRF(tf.keras.Model):\n",
        "  def init_model(self, D=8, W=256):\n",
        "    relu = tf.keras.layers.ReLU()    \n",
        "    dense = lambda W=W, act=relu : MLP(W, activation=act, H=self.H, CR=self.CR)\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(3 + 3*2*L_embed)) \n",
        "    outputs = inputs\n",
        "    for i in range(D):\n",
        "        outputs = dense()(outputs)\n",
        "        if i%4==0 and i>0:\n",
        "            outputs = tf.concat([outputs, inputs], -1)\n",
        "    outputs = dense(4, act=None)(outputs)\n",
        "    \n",
        "    return inputs, outputs\n",
        "  \n",
        "\n",
        "\n",
        "  def __init__(self, H=10, CR=False):\n",
        "    self.H = H\n",
        "    self.CR = CR\n",
        "    inputs, outputs = self.init_model()\n",
        "    super(neRF, self).__init__(inputs=inputs, outputs=outputs)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    return super(neRF, self).call(inputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clustering function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svp8U-3rqmmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "N = C if input data has size (D,C)\n",
        "H = number of hash functions (hyperplanes)\n",
        "\"\"\"\n",
        "class CLS():\n",
        "  def __init__(self, N, H):\n",
        "    self.set(N, H)\n",
        "\n",
        "  def hash(self, inputs):\n",
        "    \"\"\"\n",
        "    Given input (D,C)\n",
        "    returns outputs matrix (2^H, C)\n",
        "    \"\"\"\n",
        "    digest = tf.matmul(inputs, self.hash_mat)\n",
        "    digest = tf.cast(tf.greater(digest, 0), dtype=tf.int32)\n",
        "    keys = tf.reshape(tf.reduce_sum(digest * self.i, axis=1), (inputs.shape[0], 1))\n",
        "    keys = tf.concat([keys, tf.reshape(tf.range(0,inputs.shape[0]),(inputs.shape[0],1))], axis=1)\n",
        "    keys = tf.cast(keys, dtype=tf.int64)\n",
        "    self.st = tf.sparse.SparseTensor(keys, values=tf.ones(keys.shape[0]), dense_shape=(2**self.H, inputs.shape[0]))\n",
        "    centroids = tf.math.divide_no_nan(tf.sparse.sparse_dense_matmul(self.st, inputs), tf.reshape(tf.sparse.reduce_sum(self.st, axis=1), (self.cluster_centers.shape[0],1)))\n",
        "    self.cluster_centers = (self.cluster_centers + centroids) / 2\n",
        "    return self.cluster_centers\n",
        "  \n",
        "  def dehash(self, inputs):\n",
        "    return tf.sparse.sparse_dense_matmul(tf.sparse.transpose(self.st), inputs)\n",
        "  \n",
        "  def set(self, N, H=10):\n",
        "    self.H = H\n",
        "    self.hash_mat = tf.keras.utils.normalize(tf.random.uniform((N,self.H), minval=-1, maxval=1), axis=0)\n",
        "    self.cluster_centers = tf.zeros((2**H, N))\n",
        "    self.i = 2 ** tf.range(0, H)\n",
        "\n",
        "\"\"\"\n",
        "Decorated with @tf.function\n",
        "\"\"\"\n",
        "@tf.function\n",
        "def hash(self, inputs, hash_mat, idx, H, cluster_centers):\n",
        "    \"\"\"\n",
        "    Given input (D,C)\n",
        "    returns outputs matrix (2^H, C)\n",
        "    \"\"\"\n",
        "    digest = tf.matmul(inputs, hash_mat)\n",
        "    digest = tf.cast(tf.greater(digest, 0), dtype=tf.int32)\n",
        "    keys = tf.reshape(tf.reduce_sum(digest * i, axis=1), (inputs.shape[0], 1))\n",
        "    keys = tf.concat([keys, tf.reshape(tf.range(0,inputs.shape[0]),(inputs.shape[0],1))], axis=1)\n",
        "    keys = tf.cast(keys, dtype=tf.int64)\n",
        "    st = tf.sparse.SparseTensor(keys, values=tf.ones(keys.shape[0]), dense_shape=(2**H, inputs.shape[0]))\n",
        "    centroids = tf.math.divide_no_nan(tf.sparse.sparse_dense_matmul(st, inputs), tf.reshape(tf.sparse.reduce_sum(st, axis=1), (cluster_centers.shape[0],1)))\n",
        "    cluster_centers = (cluster_centers + centroids) / 2\n",
        "    return cluster_centers, st\n",
        "  \n",
        "def dehash(self, inputs, st):\n",
        "    return tf.sparse.sparse_dense_matmul(tf.sparse.transpose(st), inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrFk9pXOZSo7",
        "colab_type": "text"
      },
      "source": [
        "# Training function\n",
        "render_rays() takes the model and a ray as input. Draw N_samples of points alone the ray. Run the network on these points, use the output to compute the the color and opacity(density). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pEHvtdWZSo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def render_rays_profile(network_fn, rays_o, rays_d, near, far, N_samples, cls=None rand=False):\n",
        "    pr = cProfile.Profile()\n",
        "    pr2 = cProfile.Profile()\n",
        "    \n",
        "    # Compute 3D query points\n",
        "    z_vals = tf.linspace(near, far, N_samples) \n",
        "    if rand:\n",
        "      z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
        "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
        "    \n",
        "    # Run network\n",
        "    pts_flat = tf.reshape(pts, [-1,3])\n",
        "    pts_flat = embed_fn(pts_flat)\n",
        "    \n",
        "      \n",
        "    cond = tf.cast(cls!=None, tf.bool)\n",
        "    def hashable():\n",
        "      hashed = cls.hash(pts_flat)\n",
        "      raw = network_fn(hashed)\n",
        "      return clus.dehash(raw)\n",
        "    def normal():\n",
        "      return network_fn(pts_flat) \n",
        "    \n",
        "    pr.enable()\n",
        "    raw = tf.cond(cond, hashable, normal)\n",
        "    pr.disable()\n",
        "    ps = pstats.Stats(pr).sort_stats('tottime').dump_stats(\"[Profile]forward\")\n",
        "    \n",
        "    raw = tf.reshape(raw, list(pts.shape[:-1]) + [4])\n",
        "    \n",
        "    # Compute opacities and colors\n",
        "    sigma_a = tf.nn.relu(raw[...,3])\n",
        "    rgb = tf.math.sigmoid(raw[...,:3]) \n",
        "    \n",
        "    # Do volume rendering\n",
        "    pr2.enable()\n",
        "    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[...,:1].shape)], -1) \n",
        "    alpha = 1.-tf.exp(-sigma_a * dists)  \n",
        "    weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
        "    \n",
        "    rgb_map = tf.reduce_sum(weights[...,None] * rgb, -2) \n",
        "    depth_map = tf.reduce_sum(weights * z_vals, -1) \n",
        "    acc_map = tf.reduce_sum(weights, -1)\n",
        "    pr2.disable()\n",
        "    ps = pstats.Stats(pr2).sort_stats('tottime').dump_stats(\"[Profile]rendering\")\n",
        "    \n",
        "    return rgb_map, depth_map, acc_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QGCTIAcK34o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, cls=None, rand=False):\n",
        "    # Compute 3D query points\n",
        "    z_vals = tf.linspace(near, far, N_samples) \n",
        "    if rand:\n",
        "      z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
        "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
        "    \n",
        "    # Run network\n",
        "    pts_flat = tf.reshape(pts, [-1,3])\n",
        "    pts_flat = embed_fn(pts_flat)\n",
        "    \n",
        "    cond = tf.cast(cls!=None, tf.bool)\n",
        "    def hashable():\n",
        "      hashed = cls.hash(pts_flat)\n",
        "      raw = network_fn(hashed)\n",
        "    def normal():\n",
        "      return network_fn(pts_flat) \n",
        "    \n",
        "\n",
        "    raw = tf.cond(cond, hashable, normal)\n",
        "    raw = tf.reshape(raw, list(pts.shape[:-1]) + [4])\n",
        "\n",
        "    # Compute opacities and colors\n",
        "    sigma_a = tf.nn.relu(raw[...,3])\n",
        "    rgb = tf.math.sigmoid(raw[...,:3]) \n",
        "    \n",
        "    \n",
        "    # Do volume rendering\n",
        "    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[...,:1].shape)], -1) \n",
        "    alpha = 1.-tf.exp(-sigma_a * dists)  \n",
        "    weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
        "    \n",
        "    rgb_map = tf.reduce_sum(weights[...,None] * rgb, -2) \n",
        "    depth_map = tf.reduce_sum(weights * z_vals, -1) \n",
        "    acc_map = tf.reduce_sum(weights, -1)\n",
        "    \n",
        "    return rgb_map, depth_map, acc_map\n",
        "\n",
        "def render_rays_original(network_fn, rays_o, rays_d, near, far, N_samples, rand=False):\n",
        "    # Compute 3D query points\n",
        "    z_vals = tf.linspace(near, far, N_samples) \n",
        "    if rand:\n",
        "      z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
        "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
        "    \n",
        "    # Run network\n",
        "    pts_flat = tf.reshape(pts, [-1,3])\n",
        "    pts_flat = embed_fn(pts_flat)\n",
        "    \n",
        "    raw = network_fn(pts_flat) \n",
        "    raw = tf.reshape(raw, list(pts.shape[:-1]) + [4])\n",
        "\n",
        "    # Compute opacities and colors\n",
        "    sigma_a = tf.nn.relu(raw[...,3])\n",
        "    rgb = tf.math.sigmoid(raw[...,:3]) \n",
        "    \n",
        "    \n",
        "    # Do volume rendering\n",
        "    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[...,:1].shape)], -1) \n",
        "    alpha = 1.-tf.exp(-sigma_a * dists)  \n",
        "    weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
        "    \n",
        "    rgb_map = tf.reduce_sum(weights[...,None] * rgb, -2) \n",
        "    depth_map = tf.reduce_sum(weights * z_vals, -1) \n",
        "    acc_map = tf.reduce_sum(weights, -1)\n",
        "    \n",
        "    return rgb_map, depth_map, acc_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMXlbkuyZSpE",
        "colab_type": "text"
      },
      "source": [
        "# Render a single image of the scene "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LheKIbiZSpF",
        "colab_type": "code",
        "outputId": "f3a3da4a-5015-4dda-c992-0357fde3dc8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        }
      },
      "source": [
        "import time, cProfile, pstats\n",
        "model = init_model()\n",
        "#model = neRF()\n",
        "optimizer = tf.keras.optimizers.Adam(5e-4)\n",
        "\n",
        "N_samples = 64\n",
        "N_iters = 2000\n",
        "psnrs = []\n",
        "iternums = []\n",
        "losses = []\n",
        "i_plot = 50\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "log = False\n",
        "profile = False\n",
        "\n",
        "pr = cProfile.Profile()\n",
        "pr.enable()\n",
        "\n",
        "h = 10\n",
        "clus = CLS2(39, h)\n",
        "\n",
        "#t = time.time()\n",
        "for i in range(N_iters+1):\n",
        "    img_i = np.random.randint(images.shape[0])\n",
        "    target = images[img_i]\n",
        "    pose = poses[img_i]\n",
        "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
        "    with tf.GradientTape() as tape:\n",
        "        if (i%10==0): # run with clustering every 10th iteration\n",
        "          rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples,cls=clus, rand=True)\n",
        "        else:\n",
        "          rgb, depth, acc = render_rays_original(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples, rand=True)\n",
        "        loss = tf.reduce_mean(tf.square(rgb - target))\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "\n",
        "    if i%i_plot==0 and log:\n",
        "        \n",
        "        print(i, (time.time() - t) / i_plot, 'secs per iter')\n",
        "        \n",
        "        if profile:\n",
        "          p = pstats.Stats('[Profile]forward')\n",
        "          p.strip_dirs().sort_stats('tottime').print_stats(.01)\n",
        "\n",
        "          p = pstats.Stats('[Profile]rendering')\n",
        "          p.strip_dirs().sort_stats('tottime').print_stats(.01)\n",
        "\n",
        "          p = pstats.Stats('[Profile]backprop')\n",
        "          p.strip_dirs().sort_stats('tottime').print_stats(.01)\n",
        "        \n",
        "        # Render the holdout view for logging\n",
        "        \n",
        "        rays_o, rays_d = get_rays(H, W, focal, testpose)\n",
        "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n",
        "        loss = tf.reduce_mean(tf.square(rgb - testimg))\n",
        "        psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n",
        "        psnrs.append(psnr.numpy())\n",
        "        losses.append(loss.numpy())\n",
        "        iternums.append(i)\n",
        "        print(\"PSNR: {}\".format(psnr))\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.subplot(131)\n",
        "        plt.imshow(rgb)\n",
        "        plt.title(f'Iteration: {i}')\n",
        "        plt.subplot(132)\n",
        "        plt.plot(iternums, psnrs)\n",
        "        \n",
        "        plt.title('PSNR')\n",
        "        plt.subplot(133)\n",
        "        plt.plot(iternums, losses)\n",
        "        plt.title('Loss')\n",
        "        plt.grid()\n",
        "        if (i%100==0):\n",
        "          plt.savefig(\"outputs_{}.png\".format(i))\n",
        "        plt.show()\n",
        "\n",
        "        t = time.time()\n",
        "\n",
        "pr.disable()\n",
        "ps = pstats.Stats(pr).sort_stats('tottime').dump_stats(\"[Profile]training\")\n",
        "p = pstats.Stats('[Profile]training')\n",
        "p.strip_dirs().sort_stats('tottime').print_stats(.01)\n",
        "model.save('nerf_default')\n",
        "\n",
        "# Test\n",
        "testimg, testpose = images[102], poses[102]\n",
        "img_i = np.random.randint(images.shape[0])\n",
        "rays_o, rays_d = get_rays(H, W, focal, testpose)\n",
        "rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n",
        "loss = tf.reduce_mean(tf.square(rgb - testimg))\n",
        "psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n",
        "print(\"PSNR: {}\".format(psnr))\n",
        "print(\"Loss: {}\".format(loss))\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.imshow(rgb)\n",
        "plt.savefig(\"output.png\")\n",
        "plt.show()\n",
        "print('Done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITQTqtT1IKtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python38264bit5c11b25ccdc540ceb1df2efcccf696e8",
      "display_name": "Python 3.8.2 64-bit"
    },
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}